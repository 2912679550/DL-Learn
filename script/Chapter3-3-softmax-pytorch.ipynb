{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fc86ab",
   "metadata": {},
   "source": [
    "# Softmax的简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fcef04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a86e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_data = torchvision.datasets.FashionMNIST(\n",
    "    root='../data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root='../data', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "train_iter = DataLoader(train_data , batch_size=batch_size, shuffle=True)\n",
    "test_iter = DataLoader(test_data , batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eeaf83",
   "metadata": {},
   "source": [
    "softmax的回归输出层本质就是一个全连接层，为了实现模型，其实只需要一个线性层即可，其输入为784，输出为10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef46b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net= nn.Sequential(nn.Flatten() , nn.Linear(784, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.normal_(m.weight , std = 0.01)\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a50ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters() , lr = 0.1)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7363dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使能GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n",
    "loss = loss.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a37e9d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 100, loss 0.4363\n",
      "epoch 1, step 200, loss 0.4708\n",
      "epoch 1, test acc 0.8346\n",
      "epoch 2, step 100, loss 0.4964\n",
      "epoch 2, step 200, loss 0.4582\n",
      "epoch 2, test acc 0.8314\n",
      "epoch 3, step 100, loss 0.4245\n",
      "epoch 3, step 200, loss 0.4239\n",
      "epoch 3, test acc 0.8355\n",
      "epoch 4, step 100, loss 0.4638\n",
      "epoch 4, step 200, loss 0.3814\n",
      "epoch 4, test acc 0.8354\n",
      "epoch 5, step 100, loss 0.4290\n",
      "epoch 5, step 200, loss 0.3722\n",
      "epoch 5, test acc 0.8398\n",
      "epoch 6, step 100, loss 0.5360\n",
      "epoch 6, step 200, loss 0.4015\n",
      "epoch 6, test acc 0.8309\n",
      "epoch 7, step 100, loss 0.3839\n",
      "epoch 7, step 200, loss 0.5361\n",
      "epoch 7, test acc 0.8329\n",
      "epoch 8, step 100, loss 0.4683\n",
      "epoch 8, step 200, loss 0.4952\n",
      "epoch 8, test acc 0.8388\n",
      "epoch 9, step 100, loss 0.4076\n",
      "epoch 9, step 200, loss 0.4453\n",
      "epoch 9, test acc 0.8209\n",
      "epoch 10, step 100, loss 0.4220\n",
      "epoch 10, step 200, loss 0.3938\n",
      "epoch 10, test acc 0.8393\n"
     ]
    }
   ],
   "source": [
    "# 手动书写训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    train_step = 0\n",
    "    net.train()\n",
    "    for data in train_iter:\n",
    "        train_step += 1\n",
    "        trainer.zero_grad()\n",
    "        X, y = data\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y).mean()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        if train_step % 100 == 0:\n",
    "            print(f'epoch {epoch + 1}, step {train_step}, loss {l.item():.4f}')\n",
    "    # 测试模型\n",
    "    net.eval()\n",
    "    test_step = 0\n",
    "    acc_num = 0\n",
    "    all_num = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_iter:\n",
    "            test_step += 1\n",
    "            X, y = data\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            y_hat = y_hat.argmax(dim=1)\n",
    "            cmp = y_hat.type(y.dtype) == y\n",
    "            acc_num += cmp.sum().item()\n",
    "            all_num += y.shape[0]\n",
    "    print(f'epoch {epoch + 1}, test acc {acc_num / all_num:.4f}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
